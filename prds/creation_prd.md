# Product Requirements Document: Synai Prompt & Context Factory (SPCF))

**Version:** 1.0
**Date:** May 24, 2025
**Author/Stakeholder:** rickhallett
**Project Goal:** To create a Python-based utility for managing the creation, personalization, lifecycle, and tracking of prompts and contextual data for the "Synai" AI persona, prioritizing utility, flexibility, and minimal dependencies.

### 1. Introduction & Vision

The Synai Prompt & Context Factory (SPCF) will serve as the backbone for managing personalized interactions with the Synai AI assistant. As we iterate on prompt engineering and user-specific needs, a systematic way to generate, store, and update prompts and their associated contexts is crucial. This factory will automate the organization and generation of these assets, log all operations for traceability, and provide a flexible framework for evolving interaction pipelines. The vision is an agile system that supports rapid prototyping and refinement of LLM-driven interactions.

### 2. Goals

*   **G1: User Organization:** Systematically organize all data related to individual users.
*   **G2: Prompt Automation:** Automate the generation of initial and pre-loaded (seed) prompts for users.
*   **G3: Context Management:** Provide a clear way to ingest and utilize pre-existing user context.
*   **G4: Traceability:** Maintain a comprehensive log of all factory operations.
*   **G5: Flexibility & Extensibility:** Design for easy modification and addition of new operational pipelines.
*   **G6: Iteration Support:** Facilitate the storage and use of feedback for prompt refinement.
*   **G7: Minimal Dependencies:** Keep the core factory logic free of external dependencies beyond standard Python and LLM interaction mechanisms (which are treated as external services).

### 3. Target Users

*   **Primary:** Developers/Prompt Engineers (e.g., yourself) managing and iterating on the Synai AI system.
*   **Secondary (Indirect):** The Synai AI system itself, which will consume the prompts generated by this factory.

### 4. Assumptions

*   The base XML prompt templates (Synai Assessment, Preloader, Designer, Interactive modes, Dev mode toggles) are pre-defined and available.
*   LLM interactions (e.g., running the "Synai Designer" prompt) are handled by an external mechanism/service. The SPCF prepares inputs for and processes outputs from these LLM calls but does not directly execute them with a specific SDK.
*   The primary use case is local development and management.

### 5. Proposed Features & Requirements

#### 5.1. Core System & Setup

*   **F1.1: Project Initialization:**
    *   **R1.1.1:** The system should be able to initialize its required base directory structure (`data/`, `data/users/`, `base_prompts/`) if it doesn't exist upon first run or via a setup command.
    *   **R1.1.2:** The `base_prompts/` directory will be the source for all master prompt templates.
*   **F1.2: Configuration (Basic):**
    *   **R1.2.1:** Key paths (e.g., to `data/`, `base_prompts/`, SQLite DB file) should be easily configurable, perhaps via constants at the top of relevant modules or a simple configuration file (`config/settings.yaml` - optional for v1).

#### 5.2. User Management

*   **F2.1: User Creation & Identification:**
    *   **R2.1.1:** Implement a function `create_user(user_identifier: str) -> str` that:
        *   Takes a human-readable `user_identifier` (e.g., "john_doe_email").
        *   Generates a unique `user_id` (e.g., by hashing `user_identifier` + current timestamp).
        *   Creates the standard user directory structure under `data/users/[user_id]/` (including `context/`, `prompts/`, `seeds/`, `feedback/`, `interaction_dumps/`).
        *   Returns the generated `user_id`.
    *   **R2.1.2:** Ensure `user_id` generation is robust enough to avoid collisions.
*   **F2.2: User Path Retrieval:**
    *   **R2.2.1:** Implement a function `get_user_paths(user_id: str) -> dict` that returns a dictionary of all standard sub-directory paths for a given `user_id`.

#### 5.3. Prompt Management

*   **F3.1: Base Prompt Loading:**
    *   **R3.1.1:** Implement `load_base_prompt(prompt_template_name: str) -> str` to read the content of an XML prompt template from the `base_prompts/` directory.
*   **F3.2: User-Specific Prompt Generation:**
    *   **R3.2.1:** Implement `generate_user_assessment_prompt(user_id: str) -> str (file_path)`:
        *   Loads the `synai_assessment.xml` base prompt.
        *   Generates a unique filename for this prompt (e.g., `assessment_prompt_[user_id_short_hash]_[timestamp_hash].xml`).
        *   (Optional for v1, but good for future) Embeds the `user_id` or the unique prompt filename within the XML content itself (e.g., as a comment or a specific metadata tag if the Synai prompt supports it) for traceability.
        *   Saves the generated prompt to `data/users/[user_id]/prompts/`.
        *   Returns the full path to the saved prompt file.
*   **F3.3: Saving General Prompts:**
    *   **R3.3.1:** Implement `save_user_prompt(user_id: str, prompt_filename: str, prompt_content: str, subfolder: str = "prompts") -> str (file_path)` to save arbitrary prompt content to a specified subfolder within the user's directory. This will be used by the "Synai Designer" output processing.

#### 5.4. Context Processing

*   **F4.1: Context Aggregation:**
    *   **R4.1.1:** Implement `get_user_context_string(user_id: str) -> str`:
        *   Reads all files (e.g., `.txt`, `.md`) from `data/users/[user_id]/context/`.
        *   Concatenates their content into a single string, possibly with simple separators or headers indicating the source file.
        *   Returns the aggregated context string.

#### 5.5. LLM Orchestration (Preparation & Post-Processing)

*   **F5.1: "Synai Designer" Flow Support:**
    *   **R5.1.1:** Implement `prepare_designer_llm_input(context_string: str) -> str`:
        *   Loads the `synai_designer.xml` base prompt.
        *   Integrates the `context_string` into the appropriate placeholder within the designer prompt structure (or appends it if the designer prompt is designed to expect raw context after its instructions).
        *   Returns the complete string ready to be sent to an LLM running the Synai Designer.
    *   **R5.1.2:** Implement `process_designer_llm_output(designer_llm_response_xml_str: str, user_id: str) -> str (file_path)`:
        *   Takes the XML string response from the Synai Designer LLM.
        *   Parses/validates it to ensure it's the expected Seed Prompt XML.
        *   Generates a unique filename for the seed prompt (e.g., `seed_prompt_[user_id_short_hash]_[timestamp_hash].xml`).
        *   Saves this seed prompt to `data/users/[user_id]/seeds/` using `save_user_prompt`.
        *   Returns the full path to the saved seed prompt file.

#### 5.6. Operational Ledger (SQLite Database)

*   **F6.1: Database Setup:**
    *   **R6.1.1:** Implement `setup_database(db_path: str)`:
        *   Connects to the SQLite database at `db_path`.
        *   Creates the `operations_log` table if it doesn't exist, based on the schema defined in the architecture (id, timestamp, user_id, pipeline_name, operation_type, input_params_json, output_ref_json, status, notes).
*   **F6.2: Operation Logging:**
    *   **R6.2.1:** Implement `log_operation(db_path: str, user_id: str, operation_type: str, pipeline_name: str = None, input_params: dict = None, output_ref: dict = None, status: str = "SUCCESS", notes: str = None)`:
        *   Inserts a new record into the `operations_log` table.
        *   `input_params` and `output_ref` should be converted to JSON strings for storage.
*   **F6.3: Operation Retrieval (Basic):**
    *   **R6.3.1:** Implement `get_operations_for_user(db_path: str, user_id: str) -> list[dict]` to retrieve all log entries for a specific user.

#### 5.7. Pipelines

*   **F7.1: Pipeline Definition Framework (Conceptual):**
    *   **R7.1.1:** Pipelines will be Python functions that call a sequence of the above-defined manager/processor functions.
    *   **R7.1.2:** Each pipeline function should accept necessary initial parameters (e.g., `user_identifier`, `db_path`) and orchestrate the flow, including logging operations.
*   **F7.2: Specific Pipeline Implementations:**
    *   **R7.2.1: `pipeline_onboard_new_user_no_context(user_identifier: str, db_path: str) -> str (user_id)`:**
        1.  Calls `user_manager.create_user`.
        2.  Calls `prompt_manager.generate_user_assessment_prompt`.
        3.  Calls `db_manager.log_operation` for user creation and prompt generation.
        4.  Returns `user_id`.
    *   **R7.2.2: `pipeline_onboard_user_with_context_to_seed(user_identifier: str, db_path: str) -> str (user_id)`:**
        1.  Calls `user_manager.create_user` (or gets existing user_id).
        2.  User manually places context files into `data/users/[user_id]/context/`.
        3.  Calls `context_processor.get_user_context_string`.
        4.  Calls `llm_orchestrator.prepare_designer_llm_input`.
        5.  *Logs an operation: "DESIGNER_INPUT_PREPARED", status "PENDING_LLM", with the input string or reference.*
        6.  **(External LLM Call for Synai Designer occurs here)**
        7.  This pipeline might need to be split or have a way to resume. For v1, the output of the designer LLM will be manually passed to a subsequent function/pipeline step.
        8.  `pipeline_process_seed_from_designer_output(user_id: str, designer_llm_response_xml_str: str, db_path: str) -> str (seed_prompt_path)`:
            *   Calls `llm_orchestrator.process_designer_llm_output`.
            *   Logs "SEED_PROMPT_GENERATED" operation.
            *   Returns path to the seed prompt.
*   **F7.3: Feedback Ingestion Structure:**
    *   **R7.3.1:** The system only needs to ensure the `feedback/` directory exists for users. The process of *using* feedback is out of scope for SPCF v1 core but the structure should support it.

#### 5.8. Utility Functions

*   **F8.1: File I/O:**
    *   **R8.1.1:** Implement `ensure_dir_exists(path: str)`.
    *   **R8.1.2:** Implement `read_file(path: str) -> str`.
    *   **R8.1.3:** Implement `write_file(path: str, content: str)`.
*   **F8.2: Hashing:**
    *   **R8.2.1:** Implement `generate_hash(data_string: str, length: int = 8) -> str` (e.g., using SHA256 and truncating).

#### 5.85 Scripts

*   **F8.3: Scripts:**
    *   **R8.3.1:** A series of scripts to manage common factory operations:
### 6. Non-Functional Requirements

*   **NF1: Usability:** The system should be usable via Python scripts importing its modules and functions. A CLI is a "nice-to-have" for v2.
*   **NF2: Maintainability:** Code should be well-organized into modules with clear responsibilities, include comments, and follow Python best practices (PEP8).
*   **NF3: Reliability:** File operations should handle basic exceptions (e.g., file not found if a base prompt is missing). Database operations should be robust.
*   **NF4: Performance:** For the expected scale (local development, likely hundreds to a few thousand users/prompts over time), standard file I/O and SQLite performance will be sufficient. Not a primary concern for v1.
*   **NF5: Security:** User identifiers might be sensitive. Hashing provides some obfuscation. The system operates locally, so standard local file system security applies. No web-facing components in v1.
*   **NF6: Testability:** Functions should be designed to be easily unit-testable where possible (e.g., pure functions, dependency injection for `db_path`).

### 7. Out of Scope for Version 1.0

*   Direct LLM API calls or SDK integration (SPCF prepares inputs/processes outputs).
*   A graphical user interface (GUI) or web interface.
*   Advanced prompt versioning systems.
*   Automated feedback analysis and prompt refinement loops.
*   Real-time multi-user capabilities or server-based deployment.
*   Complex configuration management beyond basic path settings.
*   Automated testing framework implementation (though design should support it).

### 8. Success Metrics

*   **SM1:** Successful creation and organization of directories for 10 test users.
*   **SM2:** Successful generation of assessment prompts for these 10 users.
*   **SM3:** Successful aggregation of context from sample files for 3 users.
*   **SM4:** Successful generation of "Synai Designer LLM input" for these 3 users.
*   **SM5:** Successful processing of mock "Synai Designer LLM output" (seed prompts) and saving them correctly for these 3 users.
*   **SM6:** All operations (user creation, prompt generation, seed creation) are correctly logged in the SQLite database with appropriate details.
*   **SM7:** The core modules (`user_manager`, `prompt_manager`, `context_processor`, `db_manager`) are implemented and their primary functions are operational.

### 9. Future Considerations (Post V1.0)

*   CLI interface for easier operation.
*   Integration with a task queue for handling LLM calls asynchronously.
*   Automated prompt versioning and rollback.
*   More sophisticated feedback ingestion and analysis pipelines.
*   Integration with Git for managing `base_prompts/` and potentially user-specific prompt versions.
